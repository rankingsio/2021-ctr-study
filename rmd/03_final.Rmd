---
title: "CTR study"
author: |
  | Lead: Chris Dreyer ([rankings.io](https://rankings.io/))
  | Support: François Delavy & Daniel Kupka ([frontpagedata.com](https://frontpagedata.com/))
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    theme: paper
    highlight: kate
    # code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    keep_md: true # keep the intermediary files, including the plots as .png
editor_options:
  chunk_output_type: console
---


<style>
.list-group-item.active, .list-group-item.active:hover, .list-group-item.active:focus {
background-color: #D21D5C;
border-color: #D21D5C;
}

body {
font-family: 'Alegreya Sans', sans-serif;
color: #333333;
font-size: 18px;
}

h1 {
font-weight: bold;
font-size: 28px;
}

h1.title {
font-size: 30px;
color: #111111;
}

h2 {
font-size: 24px;
}

h3 {
font-size: 18px;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.showtext = TRUE,
                      dpi = 720
                      # dev = "svg" # for this report, using SVGs creates a much larger HTML! (75MB VS 3 MB)
                      )
knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark = ",", small.mark = ",", scientific = F)
})
# plots <- readRDS(here::here("proc_data/plots.RDS"))
# include_png <- function(item) {
#   knitr::include_graphics(
#     here::here("plots", "png", paste0(make.names(item), ".png")))
#   } 

required_packages <- c("tidyverse", "here", "colorspace", "pdftools", "kableExtra", "ggrepel", "ggiraph", "lubridate", "htmlwidgets")
# required_packages <- c("tidyverse", "readxl", "ggthemes", "hrbrthemes", "extrafont", "plotly", "scales", "stringr", "gganimate", "here", "tidytext", "sentimentr", "scales", "DT", "here", "sm", "mblm", "glue", "fs", "knitr", "rmdformats", "janitor", "urltools", "colorspace", "pdftools", "showtext")

for(i in required_packages) { 
  if(!require(i, character.only = T)) {
    #  if package is not existing, install then load the package
    install.packages(i, dependencies = T)
    require(i, character.only = T)
  }
}


## save plots?
save <- FALSE
#save <- FALSE

## quality of png's
dpi <- 750


## theme updates; please adjust to client´s website

base_size = 10
r_col <- "#D21D5C" ## highlight color of rankings.io

my_theme <- function(){
  theme_minimal() +
    theme(
      plot.margin = margin(30, 30, 30, 30),
      plot.background = element_rect(color = "white",
                                     fill = "white"),
      plot.title = element_text(size = 1.4 * base_size),
      plot.title.position = "plot",
      plot.subtitle = element_text(size = base_size),
      plot.caption = element_text(color = "grey40",
                                  size = 0.9 * base_size),
      plot.caption.position = "plot",
      strip.text = element_text(size =  base_size,
                                  face = "bold"),
      axis.line.x = element_line(color = "black",
                                 size = .8),
      axis.line.y = element_line(color = "black",
                                 size = .8),
      axis.title.x = element_text(size =  base_size,
                                  face = "bold"),
      axis.title.y = element_text(size = base_size,
                                  face = "bold"),
      axis.text = element_text(size = 0.9 * base_size,
                               color = "black",
                               face = "bold"),
      # axis.ticks = element_blank(),
      axis.ticks = element_line(color = "black"),
      panel.grid.major.x = element_line(size = .4,
                                        color = "#eaeaea",
                                        linetype = "solid"),
      panel.grid.major.y = element_line(size = .4,
                                        color = "#eaeaea",
                                        linetype = "solid"),
      panel.grid.minor.x = element_line(size = .4,
                                        color = "#eaeaea",
                                        linetype = "solid"),
      panel.grid.minor.y = element_blank(),
      panel.spacing.x = unit(2, "lines"),
      panel.spacing.y = unit(1, "lines")
    )
}


## numeric format for labels
num_format <- scales::format_format(big.mark = ",", small.mark = ",", scientific = F)



# LOADING THE DATA:
results <- rio::import(here::here("proc_data", "proc_data.rds")) 
study_plan <- rio::import(here::here("proc_data", "Study_plan.csv"))
SERP_features <- rio::import(here::here("proc_data", "SERP_features.csv"))

# we merge the study plan with the results to add the info about the city:
results <- results %>% 
  mutate(date = lubridate::ymd(date)) %>% # re-convert, maybe it solves issue on Daniel computers (already done in 01_read.Rmd) 
  left_join(study_plan %>% select(Sample_ID = Sample, City)) %>%
  rowwise() %>%
  mutate(label = paste0("Sample ", Sample_ID, ", ", City))



```

# Introduction 

Does organic CTR impact SEO rankings? In this study, we want to answer this question and  test for the effect of increasing [Click-Through Rates](https://support.google.com/google-ads/answer/2615875?hl=en) (CTRs) on Google Search rankings.  
We focus on a particular niche: car accident lawyers (personal injury lawyers). We “artificially” increased the number of visits to randomly selected car accident lawyers’ websites and measured the impact on Google rankings.  
By “artificially” increasing the number of searches and visits per day, we improve the CTR, which is considered a ranking factor. As background, [see Larry Kim's post](https://moz.com/blog/does-organic-ctr-impact-seo-rankings-new-data) for a captivating discussion on the issue.



# Methodology

We launched campaigns for 60 selected personal injury lawyers’ websites with an organic traffic provider. Each campaign is “artificially” boosting the number of searches and clicks for the keyword “[city] car accident lawyer”. By doing so, we increase their CTR for 30 days.  

We collect the position (Google Search ranking) of the website daily from the traffic provider's interface. We then analyzed the position of each website throughout the study to assess the strength of the effect and effectiveness of using a traffic provider.  

One of the major caveats of this study is that we used the traffic provider both to boost the number of visits and to measure the effect of this boost. This means that we were not able to have a control group.     

Finally, it is worth noting that while the ranking of any website is expected to vary over time, we anticipate to see a possible impact when looking at many websites. This is the reason we collected 60 samples from a variety of cities and initial rankings.  

See also the [Detailed Methodology](#detailed-methodology) in the Annex.  


# Research Findings 

## Ranking Over Time

We started by visualizing the rank evolution of the 60 websites. Each line represents the ranking of a personal injury lawyer’s website for the keyword “car accident lawyer” over time. The pink line is a LOESS regression ([locally estimated scatterplot smoothing](https://en.wikipedia.org/wiki/Local_regression)) and represents a smoothed average, similar to a moving average but better.    

```{r, fig.align='center', fig.height=5, out.width='80%'}

all_ranks_over_time <- results %>%
  ggplot() +
  # geom_line(aes(x=date, y=rank, group=Sample_ID), alpha = 0.8) +
  geom_line_interactive(aes(x = date,
                            y = rank,
                            group = Sample_ID,
                            tooltip = label,
                            data_id = label),
                        color = "#888888",
                        size = 0.4
  ) +
  geom_smooth(aes(x=date, y=rank),
              color = r_col,
              size = 1.5,
              se = FALSE) +
  # geom_line(data = results %>% group_by(date) %>% summarize(avg = mean(rank, na.rm = TRUE)),
  #           aes(x=date, y=avg),
  #           color = r_col,
  #           size = 2) +
  my_theme() +
  scale_y_reverse(breaks = c(1,5,10,15,20,25,30)) +
  scale_x_date(
    breaks = c(min(results$date), "2021-03-12", "2021-03-12", "2021-03-19",
               "2021-03-26", "2021-04-02", max(results$date)),
    date_labels = "%d %b"
    # date_breaks = "1 week",
    # date_minor_breaks = "1 day"
  ) +
  labs(
    title = "Ranking Over Time" ,
    subtitle = "Ranking of all samples over time and a smoothed average (LOESS regression) in pink.\nHover on a line to highlight the evolution of a particular website."
  ) +
  xlab("Date") +
  ylab("Rank")

ranking_over_time_widget <- girafe(ggobj = all_ranks_over_time,
                                   options = list(
                                     opts_hover_inv(css = "opacity:0.1;"),
                                     opts_hover(css = "stroke-width:2;"),
                                     opts_toolbar(position = "bottomright") # change position of download button
                                   )
)
ranking_over_time_widget

# we also save the widget as HTML:
# saveWidget(ranking_over_time_widget, file = here::here("plots/ranking_over_time_widget.html"), selfcontained = FALSE)
saveWidget(ranking_over_time_widget, file = here::here("plots/ranking_over_time_widget-self_contained.html"), 
           selfcontained = TRUE)


# save the data to re-plot with Datawrapper if necessary:
data_ranking_over_time <- results
gg_object <- ggplot_build(all_ranks_over_time)
loess_data <- gg_object$data[[2]] 
# %>% mutate(x_date = as.POSIXct(x, format = "%d%m%Y %H:%M:%S", origin = as.POSIXct("1970-01-01 00:00:00"))) # NOT WORKING
# the data, but issue: 80 data points on other x-axis...
# so, we just average the LOESS by day and lose precision...
loess_data <- loess_data %>% 
  select(x, y) %>%
  mutate(y = abs(y),
         x_date = round(x, 0)) %>%
  group_by(x_date) %>%
  mutate(avg_y = mean(y, na.rm = TRUE)) %>% 
  ungroup() %>%
  select(x_date, loess = avg_y)
results_data <- gg_object$data[[1]] %>% mutate(y = abs(y), x_date = as.Date(x, origin = as.Date("1970-01-01"))) %>%
  select(y, x, tooltip, x_date) %>%
  pivot_wider(names_from = tooltip, values_from = y) %>%
  left_join(loess_data, by = c("x" = "x_date"))

rio::export(loess_data, here::here("proc_data", 'loess_data_chart1.csv'))
rio::export(results_data, here::here("proc_data", 'results_data_chart1.csv'))


# save high quality static PNG in last resort:
ggsave(filename = here::here("plots/ranking_over_time.png"), 
       plot = all_ranks_over_time,
       width = 200,
       height = 150, 
       units = "mm",
       dpi = 800)

```

We observed the following: 

* There is variability in the ranking of the websites, which will blur the picture.   
* Some websites did see an increase in their ranking over the month. Others saw a decrease.    
* The smoothed average is increasing slightly, but the effect of boosting the visits for one month for one keyword is not extreme. It seems to be around +2 positions.
* Something seems to have happened at around week 2 in the study (approximately March 19th). There is a small jump in the ranking of many websites, forming a visible pattern. Also, it seems that the variance of the ranking decreases. The picture seems to stabilize. One potential explanation could be an update in Google’s algorithm. However, the [Rank Risk Index of RankRanger](https://www.rankranger.com/google-algorithm-updates) did not detect an update on March 18th or 19th (but an unconfirmed update on March 10th). It did, on the other hand, observe increased Google SERP fluctuations on March 13th and 14th. Another alternative explanation could be that two weeks is the time needed for the Google algorithm to capture and adjust to the boost in visits.     


## Day-to-day Fluctuations

As we previously showed, there is a high variability in the rankings, even at the daily level. We can use an indexed line plot withall law firms finishing at 0 on April 5th to show the fluctuations over time. On this chart, a sample starting on March 5th at an indexed rank of, let’s say 5, gained five ranks during the study.      

```{r, fig.align='center', fig.height=5, out.width='80%'}

last_ranks <- results %>% 
  filter(date == "2021-04-05") %>%
  select(Sample_ID, last_rank = rank)

first_ranks <- results %>% 
  group_by(Sample_ID) %>%
  mutate(first_rank = dplyr::first(na.omit(rank))) %>%
  select(Sample_ID, first_rank) %>%
  distinct()

fluctuations <- results %>%
  # left_join(last_ranks) %>%
  # mutate(indexed_rank = rank - last_rank) %>%
  left_join(first_ranks) %>%
  mutate(indexed_rank = first_rank - rank) %>%
  ggplot() +
  # geom_line(aes(x=date, y=indexed_rank, group=Sample_ID), alpha = 0.8) +
  geom_line_interactive(aes(x = date,
                            y = indexed_rank,
                            group = Sample_ID,
                            tooltip = label,
                            data_id = label),
                        color = "#888888",
                        size = 0.4
  ) +
  my_theme() +
  scale_y_continuous(breaks = c(-10, -5, 0, 5, 10)) +
  # scale_y_reverse(breaks = c(1,5,10,15,20,25,30)) +
  scale_x_date(
    breaks = c(min(results$date), "2021-03-12", "2021-03-12", "2021-03-19",
               "2021-03-26", "2021-04-02", max(results$date)),
    date_labels = "%d %b"
    # date_breaks = "1 week",
    # date_minor_breaks = "1 day"
  ) +
  labs(
    title = "Fluctuations of the Rankings Over Time" ,
    # subtitle = "The rankings are indexed to the last ranking of each sample,\nshowing the variety of trajectories leading to the final rank."
    subtitle = "The rankings are indexed to the first ranking of each sample,\nshowing the variety of trajectories leading to the final rank."
  ) +
  xlab("Date") +
  ylab("Rank Gain\nindexed to start day")

fluctuations_widget <- girafe(ggobj = fluctuations,
                              options = list(
                                opts_hover_inv(css = "opacity:0.1;"),
                                opts_hover(css = "stroke-width:2;"),
                                opts_toolbar(position = "bottomright") # change position of download button
                              )
)
fluctuations_widget


# save data in wide format for Datawrapper:
fluctuations_data <- results %>%
  # left_join(last_ranks) %>%
  # mutate(indexed_rank = rank - last_rank) %>%
  left_join(first_ranks) %>%
  mutate(indexed_rank = first_rank - rank) %>%
  select(date, label, indexed_rank) %>%
  pivot_wider(names_from = label, values_from = indexed_rank)
rio::export(fluctuations_data, here::here("proc_data", 'fluctuations_data_chart2.csv'))


```

We observed again that:  

* There are large fluctuations. Some websites saw a change of 10 ranks or more over the period. Several samples saw a difference in the ranking larger than ±5 in a single day.  
* The rankings seem to fluctuate less after March 19th. The variance of the ranking for March 19th is around ±5. After that it seems to be ±3.  



## Comparing First Week to Last Week

We saw previously that there is variability in the ranking. Thus, we will compute the rank improvement  by comparing the average rank in the first week of the study to the average rank in the last week of the study in order to obtain more stable estimates.    


```{r, fig.align='center', fig.height=5, out.width='80%'}

# we compute the average of the first and last week for all samples
all_avg_start <- results %>% 
  filter(date < "2021-03-12") %>%
  group_by(Sample_ID) %>% 
  summarise(first_week = mean(rank, na.rm = TRUE))
all_avg_end <- results %>% 
  filter(date > "2021-03-29") %>%
  group_by(Sample_ID) %>% 
  summarise(last_week = mean(rank, na.rm = TRUE))

# and add it back to the results dataframe:
results_augm <- results %>%
  left_join(all_avg_start, by = 'Sample_ID') %>%
  left_join(all_avg_end, by = "Sample_ID") %>%
  # we add a column with the starting page, to see if they have distinct fates:
  mutate(
    start_page = case_when(
      first_week <= 10 ~ "Starts on 1st page",
      first_week > 10 & first_week <= 20 ~ "Starts on 2nd page",
      first_week > 20 ~ "Starts on 3rd page"
    ),
    rank_diff =  first_week - last_week
  ) %>% 
  rowwise() %>%
  mutate(
    label_diff = paste0(label, ": ", as.character(round(rank_diff, 0)), " rank(s)")
  )

# we need the week to be a column for the slope graph,
# so we need to pivot to longer format the First and Last week:
results_augm_long <- results_augm %>%
  pivot_longer(cols = ends_with("_week"), names_to = "week", values_to = "average_rank")

# a slopegraph
slopegraph <- ggplot(data = results_augm_long) +
  # geom_line_interactive(aes(x = week,
  #                           y = average_rank,
  #                           group = Sample_ID,
  #                           tooltip = label_diff,
  #                           data_id = label_diff)) +
  geom_line(aes(x = week,
                y = average_rank,
                group = Sample_ID),
            size = 0.5) +
  my_theme() +
  scale_y_reverse(breaks = c(1,5,10,15,20,25,30)) +
  labs(
    title = "Average Rank in First and Last Week",
    subtitle = "In pink, the average of all samples."
  ) + 
  # average:
  geom_line(
    data = results_augm_long %>% group_by(week) %>% summarise(avg = mean(average_rank)),
    aes(x = week, y = avg, group = 1),
    color = r_col,
    size = 1.5
  ) +
  xlab("") +
  ylab("Average Rank") + 
  geom_text(
    data = results_augm_long %>% group_by(week) %>% summarise(avg = mean(average_rank)),
    aes(x = week, y = avg, label = as.character(round(avg,1))), 
    size = 3.5, color = r_col, nudge_x = c(-0.1, 0.1), fontface = "bold"
  ) +
  scale_x_discrete(labels=c("first_week" = "First Week", "last_week" = "Last Week"))

# draw the graph:
girafe(ggobj = slopegraph,
       options = list(
         opts_hover_inv(css = "opacity:0.1;"),
         opts_hover(css = "stroke-width:2;"),
         opts_toolbar(position = "bottomright") # change position of download button
         # opts_toolbar(saveaspng = FALSE) # use this to disable toolbar aka download button
       )
)

# save high quality static PNG in last resort:
# (still need to modify subtitle)
ggsave(filename = here::here("plots/slopegraph.png"), 
       plot = slopegraph,
       width = 200,
       height = 150, 
       units = "mm",
       dpi = 800)

```

This slopegraph is very useful to highlight the difference in ranking between the first week and the last week for all the sample websites. Again, we observe that the law firms’ websites have different fates. But globally, there is nevertheless an increase of 2 ranks in the Google Search ranking.     


Because we collected several samples (60), we can test if this difference is likely due to chance (explained by intrinsic variability in the ranking of websites) or is likely a “true” effect that would show again if we were to repeat the experiment. **A paired t-test** comparing the average ranking of each law firm during the first week and last week **shows that the difference is statistically significant**. Thus, we reject the null hypothesis that the difference in mean between the first week and last week is zero in favor of the alternative hypothesis: the true difference is not equal to 0. In other words, we indeed observed a non-zero, positive increase during the study.   

```{r, echo = FALSE}

ttest_result <- t.test(results_augm$first_week,
                       results_augm$last_week,
                       paired=TRUE,
                       # alternative = "greater" # two-sided and greater shows same result.
) 

ttest_print <- tibble(
  `mean of the differences` = round(ttest_result$estimate, 3),
  `degrees of freedom` = ttest_result$parameter,
  `t statistic` = ttest_result$statistic, 
  `p value` = ttest_result$p.value, 
  `confidence interval` = paste0(
    "[",
    round(ttest_result$conf.int[1],2), 
    ", ",
    round(ttest_result$conf.int[2],2),
    "]")
)

kbl(t(ttest_print)) %>% 
  # kbl(align=rep('c', 3)) %>%
  kable_classic(full_width = F, font = 14)

```

&nbsp;  

Is this effect stronger for websites that started on the last page?


```{r, fig.align='center', fig.height=5, out.width='100%', dpi=750}

# we need to add the average as a column, 
# so that we can plot 3 different averages in the facets.
# we compute them first:
results_augm_long <- results_augm_long %>%
  group_by(week) %>%
  # we add the overall average rank:
  mutate(avg_rank_overall = mean(average_rank)) %>% 
  # we also add the average by start_page, will be useful for the faceted slopegraph below
  # much easier before transforming to long format
  ungroup() %>%
  group_by(start_page, week) %>%
  mutate(avg_rank_overall_by_page = mean(average_rank)) %>%
  ungroup()


slopegraph_by_page <- ggplot(data = results_augm_long) +
  geom_line(aes(x = week, 
                y = average_rank, 
                group = Sample_ID)) +
  labs(
    title = "Where You Start Matters: Performance of Websites on Page 1, 2, and 3. 
",
    subtitle = "The average in pink."
  ) + 
  # average:
  geom_line(
    aes(x = week, y = avg_rank_overall_by_page, group = 1),
    color = r_col,
    size = 1
  ) +
  xlab("") +
  ylab("Average Rank") + 
  geom_text(
    aes(x = week, y = avg_rank_overall_by_page, label = as.character(round(avg_rank_overall_by_page,1))),
    size = 3, color = r_col, nudge_x = c(-0.2, 0.2), fontface = "bold"
  ) +
  my_theme() +
  scale_y_reverse(breaks = c(1,5,10,15,20,25,30)) +
  scale_x_discrete(labels=c("first_week" = "First Week", "last_week" = "Last Week")) +
  facet_wrap(vars(start_page), ncol = 3)

slopegraph_by_page


ggsave(filename = here::here("plots/slopegraph_by_page.png"), 
       plot = slopegraph_by_page,
       width = 200,
       height = 130, 
       units = "mm",
       dpi = 800)


```

The positive effect is, maybe without surprise, more substantial for websites that started on the third page (+3 rank on average, only +1 for those that started already on the first page).  

# Conclusion

The Google Search ranking of the sample personal injury lawyers’ websites can vary a few ranks daytoday. Not all the websites experienced a rank increase after a month of boosting their daily number of visits after a Google Search for the keyword “car accident lawyer”.    

Nevertheless, we observe a significant positive effect of +2 ranks, on average. Moreover, this effect is larger for websites ranking initially lower (e.g., those that appear on the third page of the search results).    

There is an interesting pattern at week 2 after the start of the study. This is  when the ranking increases and stabilizes for most of the websites. Two weeks could be the time needed for the Google algorithm to capture and adjust to the boost in visits. Alternatively, it could be the date where Google changed its algorithm. However, the [Rank Risk Index of RankRanger](https://www.rankranger.com/google-algorithm-updates) did not detect an update on March 18th or 19th.  


## Discussion of the Limitation of the Study

This study has a few limitations.   

First, we increased the number of visits for only one keyword of interest ("car accident lawyer"). However, it is arguably the most important keyword, and we multiply the estimated number of daily visits by 30 (minus a bounce rate of 50%).   

Second, we could not build a control group where we would not boost the number of visits, as the traffic provider does not allow tracking the ranking of websites without visiting them. However, ranking is a zero-sum game. So if our observed samples ranked +2 positions higher (on average), it means that other websites must have ranked lower.  

Third, we use a traffic provider to both boost and measure the impact. We thus rely on its own Google Search ranking to assess the tool. We see, however, no reason to not trust the traffic provider Google Search ranking measurements.  

Finally, we acknowledge that the results of this study are specific to the samples and parameters chosen: personal injury lawyers’ websites, “car accident lawyer” as the keyword, a calibrated boost of 30 times the estimated number of daily visits, and the traffic provider parameters and bots, etc., are thus not easily generalizable.  

# Annex

## Detailed Methodology

### Website Selection

We selected 3 personal injury lawyers in 20 U.S. cities. Then, we randomly picked one website appearing on the first page, second page and third page of the results of a Google search with the keywords “[city] car accident lawyer”. We use [Ahrefs Keyword Explorer](https://ahrefs.com/keywords-explorer) for this selection. We only select websites that appeared on the first to the third page of the Google Search results (according to Ahrefs), because [SERP Empire](https://www.serpempire.com), our traffic provider, only collects the results of the first 3 pages. In the end, we obtained a sample of 60 personal injury lawyer websites in various cities and with different Google Search positions.   

### Boosting Calibration

We wanted to give each website a comparable boost in visits that looked organic. If we delivered the same boost to the personal injury website ranking 2nd in a large city where there is a lot of competition, let’s say LA, to the website ranking 28th in Fresno, we would run into two problems. First, the comparative size of the boost would be extremely different: insignificant for the top personal injury lawyer’s website in LA, but oversized for the lower ranking website in Fresno. Second, the boost in Fresno would not look “organic” and would likely be detected by Google as not being _real_ organic traffic on the website and be discarded.     

Therefore, we calibrate the boost in visits to an _estimated number of daily visits_ for each website. Here is how we do it: 

1. We multiply the _average number of daily searches_ in a city for the keywords “[_city_] car accident lawyer”, computed from the monthly volume on Google Ads, to the expected CTR of the website, computed from is Google search ranking (provided by Ahref) to a _ranking CTR factor_ (provided by the [Google Organic CTR History](https://www.advancedwebranking.com/ctrstudy/) of Advanced Web Ranking). This _ranking CTR factor_ estimates the CTR in the relation to the position in the Google Search results. Indeed, a website ranking first is much more likely to be visited than a website ranking lower. By doing so, we get the number of _estimated daily visits_.  

2. Then, we multiply the _estimated daily visits_ to a common _multiplier_, the “boost”, to obtain the calibrated number of daily keyword searches that we will add to each website with SERP Empire.

In short, for each website:

1. $\text{Ranking CTR factor} * \text{Daily Searches for keyword in city} = \text{Estimated Daily Visits}$
2. $\text{Estimated Daily Visits} * Multiplier = \text{Daily Keyword Searches}$

Where:   

* $\text{Ranking CTR factor} = \text{current ranking} * \text{CTR estimate from Google Organic CTR History study}$  
* ${Daily Searches for keyword in city}$ is computed from Google Ads monthly volume for the keyword "[_city_] car accident lawyer"
* $\text{CTR estimate from Google Organic CTR History study}$ is obtained from the _Organic + Local Pack_ ([source](https://www.advancedwebranking.com/ctrstudy/)) and is:  

```{r}
SERP_features %>%
kbl(align=rep('c', 2)) %>%
kable_classic(full_width = F, font = 10)
# kable_styling(bootstrap_options = "condensed")
```

* The $Multiplier$ is how much we multiply the $\text{Estimated Daily Visits}$: 30  

This is the study plan:  

```{r}
study_plan %>%
kbl(align=rep('c', 2)) %>%
kable_classic(full_width = F, font = 10) %>%
column_spec(2:4, background = "#FAFAFA") %>%
column_spec(7:9, background = "#FAFAFA")

```

How to read the study plan, for instance, for the first personal injury lawyer in San Antonio: We multiply the baseline number of daily searches in San Antonio for the keyword to the CTR, which depend on the ranking, to obtain the _estimated number of daily clicks_ for the sample 1. Then, we multiply this number by our multiplier to obtain the number of visits that we will pay SERP Empire to do on this website daily.    

### Traffic Provider Parameters

We use the traffic provider [SERP Empire](https://www.serpempire.com) to visit our 60 sampled websites the number of days indicated in the column _Number of Daily Searches on SERP Empire_ on the study plan above. SERP Empire bots will vary their exact number of visits around this average to make the traffic look more organic.   

In addition, we used the following SERP Empire parameters:  

* The country and region are specified
* 50% of mobile searches
* Two pages per session
* Average session duration: 60 seconds
* Bounce Rate: 50%

We boost the daily visits of the websites for one month.  


### Data Collection and Quality

From the SERP Empire dashboard, we manually collect the rank of each of the 60 websites every day.  

A few comments on the data quality:   

* Sample number 6 is removed from the analysis as the website could no longer be found by SERP Empire for the majority of the study (from 2021-03-14).    
* There are a few websites that could not easily be found by SERP Empire and needed to be modified. Thus, there are a few missing days at the beginning of the month for some samples.   
* The initial ranking obtained from Ahrefs determined the CTR ranking factor is sometimes a bit different from the ranking on SERP Empire, which is to be expected. Nevertheless, it is quite similar (it explains, for instance, why some websites ranked above  position 30).
* We boost the visits for one keyword only: “[city] car accident lawyer”.


